{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "architectural-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "average-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "excellent-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([transforms.Resize((28,28)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "accomplished-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train': ImageFolder(root = './Train', transform = image_transforms),\n",
    "       'test': ImageFolder(root = './Test', transform = image_transforms),\n",
    "       'validation': ImageFolder(root = './Validation', transform = image_transforms)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sunrise-toolbox",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'WithMask': 0, 'WithoutMask': 1},\n",
       " {'WithMask': 0, 'WithoutMask': 1},\n",
       " {'WithMask': 0, 'WithoutMask': 1})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].class_to_idx, data['test'].class_to_idx, data['validation'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "moral-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train': DataLoader(data['train'], batch_size = 100, shuffle = True),\n",
    "              'test': DataLoader(data['test'], batch_size = 20, shuffle = True),\n",
    "              'validation': DataLoader(data['validation'], batch_size = 40, shuffle = True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-console",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "interpreted-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_resnet = models.resnet50(pretrained=True)\n",
    "pretrained_resnet.fc = nn.Linear(pretrained_resnet.fc.in_features, 2)\n",
    "pretrained_resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "educated-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(pretrained_resnet.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loss = []\n",
    "validation_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "epochs = 1\n",
    "for epoch in tqdm(range(1,epochs+1)):\n",
    "        for image, label in dataloaders['train']:\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            \n",
    "            pretrained_resnet.zero_grad()\n",
    "            prediction = pretrained_resnet(image)\n",
    "            loss = criterion(prediction, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"EPOCH\", epoch)\n",
    "        print('training_loss: ', loss)\n",
    "        train_loss.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "artistic-warehouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.983\n",
      "correct  975\n",
      "total 992\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "\n",
    "for image, label in dataloaders['test']:\n",
    "    image, label = image.to(device), label.to(device)\n",
    "\n",
    "    prediction = pretrained_resnet(image)\n",
    "    #loss = loss_fun(prediction, label)\n",
    "    #print(prediction)\n",
    "    for k in range(len(prediction)):\n",
    "        #print(k)\n",
    "        #print(prediction)\n",
    "        #print(label)\n",
    "        if torch.argmax(prediction[k]) == label[k]:\n",
    "            correct+=1\n",
    "            #print(prediction[k],label[k])\n",
    "        total += 1\n",
    "\n",
    "#validation_loss.append(loss)    \n",
    "#print('validation_loss: ', loss)\n",
    "print('accuracy: ', round(correct/total, 3))\n",
    "print('correct ', correct)\n",
    "print('total', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "great-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([transforms.ToPILImage(),transforms.Resize((28,28)),\n",
    "                                      transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "labels = ['Mask', 'No Mask']\n",
    "pretrained_resnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-grocery",
   "metadata": {},
   "source": [
    "### OpenCV DNN Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "exterior-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "import cv2\n",
    "labels = ['Mask', 'No Mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFile = \"./cv2dnn/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"./cv2dnn/deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    frame = imutils.resize(frame, width=750)\n",
    "\n",
    "    # grab the frame dimensions and convert it to a blob\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
    "        (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        if confidence < 0.5:\n",
    "            continue\n",
    "\n",
    "        # compute the (x, y)-coordinates of the bounding box for the\n",
    "        # object\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        face_img = frame[startY:startY+endY,startX:startX+endX]\n",
    "        # draw the bounding box of the face along with the associated\n",
    "        # probability\n",
    "        \n",
    "        output = pretrained_resnet(transformations(face_img).unsqueeze(0).to(device))\n",
    "        predicted = torch.argmax(output\n",
    "                                )\n",
    "        #text = \"{:.2f}%\".format(confidence * 100)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "            (0, 0, 255), 2)\n",
    "        cv2.putText(frame, labels[predicted], (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "video_capture.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-manner",
   "metadata": {},
   "source": [
    "### Open CV Haarcascade method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "disabled-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "labels = ['Mask', 'No Mask']\n",
    "#cascPath = sys.argv[1]\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    face_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        face_frame,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Predicting mask label\n",
    "        face_img = frame[y:y+h,x:x+w]\n",
    "        output = pretrained_resnet(transformations(face_img).unsqueeze(0).to(device))\n",
    "        predicted = torch.argmax(output)\n",
    "        #print(output)\n",
    "        cv2.putText(frame,labels[predicted],(x,y),cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255),2)\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
